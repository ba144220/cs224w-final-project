import torch
import torch.nn.functional as F
from torch_geometric.nn import MessagePassing, global_mean_pool
from torch_geometric.utils import add_self_loops, degree


class SimpleGCNLayer(MessagePassing):
    """
    A simple GCN layer implemented using the MessagePassing base class.

    Implements the graph convolution operator from the "Semi-supervised Classification
    with Graph Convolutional Networks" <https://arxiv.org/abs/1609.02907> paper.
    """

    def __init__(self, in_channels: int, out_channels: int):
        super().__init__(aggr="add")  # "Add" aggregation (Step 5).
        self.lin = torch.nn.Linear(in_channels, out_channels)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        # x has shape [N, in_channels]
        # edge_index has shape [2, E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3: Compute normalization.
        row, col = edge_index
        deg = degree(col, x.size(0), dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float("inf")] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4-5: Start propagating messages.
        return self.propagate(edge_index, x=x, norm=norm)

    def message(self, x_j: torch.Tensor, norm: torch.Tensor) -> torch.Tensor:
        # x_j has shape [E, out_channels]
        # norm has shape [E]

        # Step 4: Normalize node features.
        return norm.view(-1, 1) * x_j


class SimpleGNN(torch.nn.Module):
    """
    A simple Graph Neural Network (GNN) model using custom MessagePassing layers
    and a prediction head for graph-level regression.

    This model is designed to work with the circuit graphs generated by net_to_graph.py.
    """

    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):
        """
        Initialize the SimpleGNN model.

        Args:
            in_channels (int): Number of input features per node (e.g., number of operation types).
            hidden_channels (int): Number of hidden units.
            out_channels (int): Number of output units for the prediction head.
        """
        super().__init__()
        self.conv1 = SimpleGCNLayer(in_channels, hidden_channels)
        self.conv2 = SimpleGCNLayer(hidden_channels, hidden_channels)

        # Prediction head
        self.head = torch.nn.Linear(hidden_channels, out_channels)

    def forward(
        self, x: torch.Tensor, edge_index: torch.Tensor, batch: torch.Tensor = None
    ) -> torch.Tensor:
        """
        Forward pass of the model.

        Args:
            x (torch.Tensor): Node feature matrix of shape [num_nodes, in_channels].
            edge_index (torch.Tensor): Graph connectivity matrix of shape [2, num_edges].
            batch (torch.Tensor, optional): Batch vector mapping each node to a graph.
                                          Shape [num_nodes]. Defaults to all zeros (single graph).

        Returns:
            torch.Tensor: Output matrix of shape [num_graphs, out_channels].
        """
        # First Graph Convolution layer
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.5, training=self.training)

        # Second Graph Convolution layer
        x = self.conv2(x, edge_index)
        x = F.relu(x)

        # Global Pooling (aggregate node embeddings to graph embedding)
        if batch is None:
            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)

        x = global_mean_pool(x, batch)  # Shape: [num_graphs, hidden_channels]

        # Prediction head
        x = self.head(x)  # Shape: [num_graphs, out_channels]

        return x


if __name__ == "__main__":
    # Example usage
    # The number of input features matches the number of OP_TYPES in net_to_graph.py (which is 9)
    in_features = 9
    hidden_dim = 16
    out_dim = 1  # Example output dimension (single value prediction)

    model = SimpleGNN(
        in_channels=in_features, hidden_channels=hidden_dim, out_channels=out_dim
    )
    print("Model Architecture:")
    print(model)

    # Create dummy data to test
    num_nodes = 10
    x = torch.randn((num_nodes, in_features))
    edge_index = torch.tensor([[0, 1, 1, 2], [1, 0, 2, 1]], dtype=torch.long)

    # Single graph (batch=None)
    out = model(x, edge_index)
    print(f"\nInput shape: {x.shape}")
    print(f"Output shape (single graph): {out.shape}")

    # Batch of 2 graphs
    batch = torch.tensor([0] * 5 + [1] * 5, dtype=torch.long)
    out_batch = model(x, edge_index, batch)
    print(f"Output shape (batch of 2): {out_batch.shape}")
